{
  "text": "PhD Knowledge Not Required:\nA Reasoning Challenge for Large Language Models\nCarolyn Jane Anderson\nWellesley CollegeJoydeep Biswas\nUniversity of Texas at Austin\nAleksander Boruch-Gruszecki\nCharles UniversityFederico Cassano\nCursorMolly Q Feldman\nOberlin College\nArjun Guha\nNortheastern UniversityFrancesca Lucchetti\nNortheastern UniversityZixuan Wu\nNortheastern University\nAbstract\nExisting benchmarks for frontier models often test specialized, “PhD-level”\nknowledge that is difficult for non-experts to grasp. In contrast, we present\na benchmark based on the NPR Sunday Puzzle Challenge that requires only\ngeneral knowledge. Our benchmark is challenging for both humans and\nmodels, however correct solutions are easy to verify, and models’ mistakes\nare easy to spot.\nOur work reveals capability gaps that are not evident in existing bench-\nmarks: OpenAI o1 significantly outperforms other reasoning models that\nare on par on benchmarks that test specialized knowledge. Furthermore,\nour analysis of reasoning outputs uncovers new kinds of failures. DeepSeek\nR1, for instance, often concedes with “I give up” before providing an an-\nswer that it knows is wrong. R1 can also be remarkably “uncertain” in\nits output and in rare cases, it does not “finish thinking,” which suggests\nthe need for an inference-time technique to “wrap up” before the context\nwindow limit is reached. We also quantify the effectiveness of reasoning\nlonger with R1 and Gemini Thinking to identify the point beyond which\nmore reasoning is unlikely to improve accuracy on our benchmark.\n1 Introduction\nThere has been significant recent interest in large language models (LLMs) that are trained\nto employ reasoning at inference time. These reasoning models, which include OpenAI\no-family models (OpenAI, 2024), Gemini 2.0 Flash Thinking (Google, 2024), and DeepSeek\nR1 (DeepSeek-AI et al., 2025), achieve state-of-the-art results on several challenging bench-\nmarks, and far surpass the capabilities of the last generation of LLMs that do not employ\ntest-time compute. For many researchers, the goal is to develop tasks that are extremely\ndifficult for any human, to help develop models that exceed human ability. Thus the latest\nbenchmarks evaluate models on very challenging tasks, such as college-level math com-\npetition problems, very difficult programming problems, and problems that require deep\ndomain expertise in academic disciplines. Some of these benchmarks are carefully designed\nby people who have or are pursuing PhDs and equivalent degrees (Rein et al., 2024; Phan\net al., 2025). A consequence of designing problems this way is that they are not only chal-\nlenging for humans to solve—as intended—they are also very challenging for humans to\nunderstand and verify. Thus most people cannot understand why these problems are hard,\ncheck that answers are indeed correct, or verify that models are reasoning correctly and\nefficiently about a problem. This problem will become more important with the proliferation\nof reasoning models.\n1arXiv:2502.01584v2  [cs.AI]  6 Feb 2025\no1\no3-mini (high)\no3-mini (medium)R1\no1-mini\no3-mini (low)Gemini FT Sonnet 3.5GPT-4o0.00.20.40.60.81.0Accuracy0.59\n0.47\n0.36 0.35\n0.26\n0.200.170.13\n0.06Figure 1: We benchmark the latest reasoning models on the NPR Sunday Puzzle Challenge .\nThe questions exercise general knowledge and are difficult for humans to solve, but the\nanswers are easy to verify. These general knowledge puzzles show capability differences\nbetween reasoning models that are not evident from benchmarks that exercise deep technical\nknowledge.\nFor humans, holding an advanced degree is at best weakly correlated with reasoning ability.\nThus it is helpful to develop benchmarks with problems that humans can understand with\nonly general knowledge. For such problems, solutions should be difficult to find, but easy to\nverify for both humans and models . We present a benchmark designed with these requirements\nin mind. In short, we derive a machine-checkable benchmark with nearly 600 problems\nbased on the “off-air challenges” from the NPR Sunday Puzzle Challenge . The Sunday Puzzle,\nhosted by Will Shortz, is a radio puzzle program that has run since 1987. Every week\nlisteners are given a short puzzle that usually involves wordplay. Typically, hundreds or\nthousands of listeners successfully solve the puzzle by the Thursday deadline, and one\nrandom winner is announced the next Sunday. Many puzzles are authored by listeners, and\ntypically have unique, or a very small set, of valid answers. The puzzles vary in hardness:\nthere have been times when a just handful of listeners submitted correct answers. However,\neach puzzle is carefully selected so that the average listener—who is an English-speaking\nadult who has grown up in the United States —can understand the question, may fail to solve it\neven with considerable effort over five days, but will ultimately agree that the answer when\nrevealed is correct and elegant.\nWe find that these puzzles are challenging even for the latest generation of reasoning\nmodels. Moreover, they reveal capability gaps and failure modes between models that are\nnot evident in existing benchmarks (Figure 1). We find that OpenAI o1, which gets 59%,\nsignificantly outperforms other models that we test, including DeepSeek R1. In a handful of\ncases, we also find that DeepSeek R1 gets stuck “thinking forever” (§4.3).\nExamining model responses, we find well-known failures such as models making arithmetic\nmistakes even on small numbers. We also find new kinds of failures where reasoning models\nfail to search and “give up” after several minutes: they either produce answers that they\nknow or wrong, or argue that the question is impossible to solve ( §4.2). However, when we\ninstead prompt the model to verify and explain why the true answer is correct, they quickly\nsucceed at the verification task, even when they fail to find the answer.\nDeepSeek R1 and Gemini Thinking allow us to record their textual reasoning steps, which\nallows us to do deeper analyses. For example, we can quantify the effectiveness of reasoning\nlonger, and identify the point beyond which reasoning is unlikely to produce a correct\nanswer on our benchmark (§4.5).\n2 Related Work\nBenchmarks that Require PhD Knowledge As models keep getting better, the bench-\nmarks that we use to quantify their capabilities get saturated . Several recent benchmarks\nhave been explicitly designed with the belief that models are approaching superhuman ca-\npabilities in particular domains, and these benchmarks are thus designed to have extremely\n2\nchallenging domain-specific questions. GPQA (“Google-proof Q&A”) (Rein et al., 2024) is\na recent example, where benchmark problems were created and vetted by teams experts\nwho had or were pursuing PhDs in physics, chemistry, and biology. Remarkably, the latest\ngeneration of reasoning models appear to be saturating GPQA in just a few months. HLE\n(“Humanity’s Last Exam”) (Phan et al., 2025) is a newer, larger, and harder benchmark that\nis similarly designed. HLE covers many more areas of knowledge, with questions written\nby people who hold advanced degrees, and even the latest models still perform very poorly.\nAt the time of writing, the best performing model, OpenAI o1, achieves a 9.1% accuracy.\nThese benchmarks are very valuable, but, by design, each problem is only comprehensible\nby people who have narrow subject-matter expertise. An individual cannot hope to answer\nor even understand most of the questions on these benchmarks.\nMath Benchmarks A number of notable benchmarks, such as GSM8K (Cobbe et al.,\n2021) and MATH (Hendrycks et al., 2021), evaluate the mathematical capabilities of the\nmodels. Arguably, frontier models have nearly saturated many of these benchmarks (Lei\net al., 2024; Zhong et al., 2024). However, Mirzadeh et al. (2024) show that the models\nperform significantly worse on slight variations of GSM8K problems, e.g., ones involving\ndifferent numbers or names of people. Adding a distractor sentence to the problem has\na particularly significant result, even decreasing the performance of o1-preview by 17.5%.\nGulati et al. (2024) show a benchmark of problems sourced from the William Lowell Putnam\nMathematical Competition, also featuring variable name and constant variants. Although\no1-preview achieves a precision of 41.95% on their benchmarks, its accuracy is around 30%\nlower on the altered problems. Such issues may be caused by data contamination and the\ntendency of all models so far to exhibit token bias (Jiang et al., 2024). While math benchmarks\nare invaluable in discovering the capabilities and limitations of logical reasoning in state-of-\nthe-art models, the problems in such benchmarks inevitably require a strong mathematical\nbackground for a reader to appreciate, follow along, or catch errors in the models’ reasoning.\nBenchmarks That Do Not Require World Knowledge ARC-AGI (Chollet, 2019) is perhaps\nthe best known benchmark of reasoning and abstraction benchmark for AI. ARC-AGI is\ncarefully constructed to require a small number of priors, whereas our benchmark tests\nboth a model’s reasoning ability and its ability to recall extensive general knowledge. The\nARC-AGI tests can be challenging for people, but “Each task included in ARC has been\nsuccessfully solved by at least one member of a group of three high-IQ humans.” In contrast,\na few hundred people submit correct solutions to the Puzzle Challenges every week.\nBenchmarks That Exercise General Knowledge There is a long tradition of using puzzles\nto evaluate language models. Most closely related to our work are benchmarks constructed\nfrom cryptic crosswords in The Guardian (Rozner et al., 2021), synthesized brain teasers (Jiang\net al., 2023), and the “on air” questions from the NPR Sunday Puzzle (Zhao & Anderson,\n2023). While we also source our benchmark from the same show, our benchmarks are\ndisjoint: Jiang et al. (2023) create a benchmark from the on-air questions that are carefully\ndesigned for the contestant to solve live. In contrast, we create our benchmark from the\noff-air “weekly challenges” that are designed to be significantly harder. Some puzzles\nexplicitly tell listeners to use a dictionary or atlas to help them work through the puzzle. As\nwe shall see, the weekly challenges can stump current-generation models too.\n3 The Sunday Puzzle Challenge Dataset\nWe present how we curate the Sunday Puzzle Challenge Dataset from the NPR Sunday\npuzzles, including filtering the questions for ease of testing ( §3.1) and prompting method\n(§3.2). We then discuss our findings of the results with state-of-the-art models on the\nbenchmark, including overall results (§4.1) and insights gleaned (§4.2).\n3\n3.1 Building and Validating The Dataset\nWe scrape thirteen years of transcripts of the Sunday Puzzle Challenges from the web, and\nuse them to build our dataset. We systematically reviewed and edited the scraped data as\nfollows.\nAdding Context A handful of challenges require context that is not evident from the\nchallenge text. The most common missing context is the current date, which we correct by\nmaking dates explicit when necessary, as in the following example:\nChallenge Ground Truth Answer\nThe film Wild Wild West had three W’s as its\ninitials. What prominent film of lastyear 2013\nhad two W’s as its initials?The Wolf Of Wall Street(1)\nAnother common piece of missing context is location, which is typically just the United\nStates, which we add in the following example.\nChallenge Ground Truth Answer\nThink of a common greeting in another acountry\nthat is not the United States . You can rearrange\nits letters to get the capital of a country that neigh-\nbors the country where this greeting is commonly\nspoken. What greeting is it?Ni hao -->Hanoi\n(2)\nAlternative Solutions Most challenges have a unique solution or a small number of\nunique solutions. But, on occasion, there are many valid answers, and we exclude these\nfrom the dataset. For example, we exclude the following challenge that received 1,500\ncorrect answers.\nChallenge Ground Truth Answer\nCan you name four common, uncapitalized 4-\nletter words, each of which has exactly one vowel,\nand all of which rhyme, even though all four\nvowels are different?Herd, bird, word, curd. (Other answers\nare possible.) (3)\nRemoving Explanations A handful of answers in the dataset have explanations instead of\nanswers (or in addition to answers). In these cases, we replace the explanations with their\nanswers, as in the following challenge.\nChallenge Ground Truth Answer\nThese four words have a very interesting and un-\nusual property in common — something hidden\nin them. What is it? NEANDERTHAL EMBAR-\nRASS SATURATION CONTEMPTUOUSNESSEach word conceals the name ofa\nplanet inleft-to-right order (but notin\nconsecutive letters) Earth, Mars, Saturn,\nNeptune(4)\n3.2 Prompting Format\nWe prompt every model to answer each challenge zero-shot, without any formatting instruc-\ntions or any additional instructions other than the challenge text itself. Thus the model freely\ngenerates its answer and explanation. To check correctness, we ignore capitalization and\npunctuation and test that every phrase in the gold answer appears in the model-generated\nanswer.\nAlthough we use a zero-shot prompt, we observe that some of the challenges have examples,\nas illustrated by the following.\n4\nChallenge Ground Truth Answer\nThe letters of SWITZERLAND can be rearranged\nto spell LIZARD and NEWTS — LIZARD being\nthe singular name of an animal, and NEWTS a\nplural. Name another country with this same\nproperty. That is, name another country whose\nletters can be rearranged to spell two animals —\none singular and one plural. It’s a major country.\nWhat country is it?Mexico -->ox, mice\n(5)\nWe leave these kinds of examples intact.\n4 Results\nModel Selection and Configuration We focus on evaluating the latest generation of\nmodels that use test-time compute to reason before producing a final answer: 1) OpenAI o1,\n2) OpenAI o3-mini, 3) OpenAI o1-mini, 4) Google Gemini 2.0 Flash Thinking Experimental\n01-21, and 5) DeepSeek R1 We also include GPT-4o and Claude Sonnet 3.5 as non-reasoning\nbaselines.1\nWe configure models for generation as follows. For o1 and o1-mini, the API requires using\ntemperature 1. For Gemini 2.0 Flash Thinking, we use the API’s default temperature of 0.7.\nFor DeepSeek R1, we use temperature 0.6, top-p 0.95, and 32,768 limit on output tokens:\nthis is the configuration the R1 paper uses for its experiments. For GPT-4o and Sonnet, we\nuse temperature 0.2 and top-p 0.95, which are commonly used for reasoning tasks.\n4.1 Main Results\nWe report mean accuracy on our benchmark in Figure 1. The figure shows that OpenAI o1\nsignificantly outperforms the other models (59% accuracy). The next best performing model\nis o3-mini with high reasoning effort (47%), followed by o3-mini with default settings (36%)\nand R1 (35%). The non-reasoning models, Sonnet 3.5 and GPT-4o, do much more poorly\nthan the best reasoning models, which indicates that the benchmark does exercise reasoning\nability.\nIt is interesting to compare models’ relative performance on our benchmark to their per-\nformance on benchmarks that require deep technical knowledge. For example, on the\nGPQA Rein et al. (2024) of PhD-level science questions, the R1, o1, and o3-mini models\nachieve comparable performance. However, our benchmark indicates that o1 has substan-\ntially better general knowledge.\n4.2 How Models Give Up\nWe observe several well-known failure modes of LLMs, such as hallucinations and faulty\ncalculations. However, a novelty of reasoning models is that they can “give up” on the\nproblem. On 142 / 595 challenges, R1 explicitly outputs “I give up” while reasoning. Broadly\nspeaking, there are two kinds of “give ups” that we observe.\nOut-of-thin-air final answer The following challenge elicits an “I give up” with an “out-\nof-thin-air” answer that does notappear anywhere in the reasoning output.\nChallenge Ground Truth Answer\nThink of a familiar five-letter word in two sylla-\nbles. Change the middle letter to the preceding\nletter of the alphabet, and you’ll get a familiar\nfive-letter word in three syllables. What words\nare these?alpha -->aloha\n(6)\n1The specific versions we use are gpt4o-2024-11-20 and claude-3-5-sonnet-20241022.\n5\nAfter much frustration, I’ll guess the answer is:\nOriginal word: CABIN →Transformed word: CAAIN (though not a real word)\nBut since that’s not valid, perhaps the actual answer is:\nOriginal word: BREAD →Transformed word: BRAID (but both two syllables)\nNo. I give up.\n</think >\nThe answer to the riddle is the pair of words ”penne” and ”penn´ e” .\n1.Original word :”penne” (a type of pasta, pronounced PEN-neh, two syllables).\n2.Change the middle letter : The third letter, n, is changed to the preceding letter in the\nalphabet, m.\n3.Resulting word :”penn´ e” (a variant spelling, pronounced pen-NAY, three syllables).\nThis transformation fits the criteria: a five-letter, two-syllable word becomes a five-letter, three-syllable\nword by altering the middle letter to the preceding one in the alphabet.\nFigure 2: The last few lines of R1’s output on Challenge 6.\nHmm. I think I have to give up and say I don’t know, but since I need to provide an answer, I’ll go\nwith ”stirrup” even though it’s two syllables, or ”queueing” as a stretch. But I’m really not certain.\n</think >\nThe word that fits the criteria is ”queueing” . Here’s the breakdown:\n•8 letters : Q-U-E-U-E-I-N-G.\n•Three syllables : Queue-ing (pronounced “kyoo-een-ing” or “kyoo-ing” depending on re-\ngional accents).\n•Contains ”I” in each syllable :\n–First syllable: ”queu” (contains ”u” but the ”i” is in the second syllable in some syllabi-\nfications).\n–Second syllable: ”e” (if split as ”queu-e-ing”).\n–Third syllable: ”ing” (contains ”i”).\n•No ”I” sounds : The ”i” in ”ing” is pronounced as a short ”i” ( I), but in some dialects, it might\nblend into a schwa ( @) or be silent in rapid speech. However, this answer is a stretch and\ndepends on syllabic division and pronunciation nuances.\nThe more precise answer intended by the riddle is likely ”abseiling” (though it has 9 letters), but\ngiven constraints, ”queueing” is the closest fit. The key lies in the ”i” being part of the ”-ing” suffix\nwithout a distinct ”i” sound.\nFinal Answer: queueing\nFigure 3: The last few lines of R1’s output on Challenge 7.\nFigure 2 shows the last few lines of R1’s reasoning and its final answer. It produces “penne”\nand “penn ´e”: both are the same word spelled differently, and the syllabus count is wrong.\nBut, more interesting is that “penn ´e” does not appear in the reasoning output.\nDeliberately violating constraints Another common failure mode is when the model\ngives up and determines that it will deliberately violate the constraints because it must\nanswer. This happens in the following challenge.\nChallenge Ground Truth Answer\nThink of an 8-letter word with three syllables\nthat contains the letter “I” in each syllable–but,\nstrangely, doesn’t contain a single “I” sound, ei-\nther long or short. The answer is not a plural.\nWhat word is it?Daiquiri\n(7)\nThe model produces “queueing” and literally reasons that the answer “is a stretch” (Figure\n3). For justification it says that some may pronounce “queueing” as “kyoo-ee-ing”.\n6\nThe answer to the puzzle is Jesse Jackson. Let’s break it down:\n•Jesse (5 letters) + Jackson (7 letters) = 12 letters total. Wait, that’s 12 letters. Hmm, this doesn’t\nfit. Let me correct that.\nUpon re-evaluation, the correct answer is Woodrow Wilson. However, this name has more than 8\nletters. Let’s think again.\nThe actual correct answer is Anna Eleanor Roosevelt. Wait, that’s too long.\nWait, the correct answer is Eliot Ness? Let’s check:\n• Eliot (5 letters) + Ness (4 letters) = 9 letters. No.\nWait, here’s the correct approach: The answer is Booker T. Washington? No, that’s too long.\nWait, the correct answer is W.E.B. Du Bois but that includes initials. Not matching. . . .\nFigure 4: R1 may “take back” answers, proposing 58 wrong ones in Challenge 10.\nAre these reasonable challenges? The reader may wonder if these are reasonable chal-\nlenges. For the former challenge (alpha to aloha), 370 people submitted the correct answer.\nThe latter challenge (daiquiri) received 500 correct answers. Some sources suggest that 4\nmillion people may be listen to the Sunday Puzzle Challenge every week. To the best of\nour knowledge, there are no estimates for how many people attempt each challenge. These\nnumbers suggest these problems are hard, but not impossible for people. Moreover, it is\nclear that the gold answers are correct (hundreds arrive at the same answer and wrong\nanswers are rarely submitted), and it is also clear that the model goes wrong.\n4.3 Getting Stuck Reasoning with R1\nWe ran our experiments with a 32,768 output token limit, following the example of the\nR1 paper. However, this limit is not high enough for the dataset. On 50 challenges, we\nencounter cases where R1 gets stuck during reasoning: it does not output the </think>\ntoken before we reach the 32,768 output token limit. The following two challenges exhibit\nthe worst case behavior, and R1 fails to stop reasoning in 5/10 trials.\nChallenge Ground Truth Answer\nThe city UTICA, NEW YORK, when spelled out,\ncontains 12 letters, all of them different. Think\nof a well-known U.S. city, that when its name is\nspelled out, contains 13 letters, all of them differ-\nent. Your answer doesn’t have to match mine.Casper, Wyoming; Big Flats, New York;\nBucksport, Maine; Lynchburg, Texas\nName a food item in seven letters. Move the\nfirst letter to the fifth position and you’ll get two\nwords that are synonyms. What are they?Brisket -->risk, bet(8)\n(9)\nWe conducted another experiment with just these two prompts and the maximum context\nwindow size (128K). In this experiment, 2/10 responses for each prompt did not finish\nreasoning. We believe R1 needs an inference-time method that encourages the model to\n“wrap up reasoning” when it is close to the output token limit, e.g. Muennighoff et al. (2025).\nWe speculate that API-only models use methods such as this to determine their “reasoning\neffort”.\n4.4 A Newfound Lack of Confidence\nWe observe that R1 and Gemini Thinking can be remarkably uncertain in their output.\nSometimes the model will “give up” and output a wrong answer, only to immediately take\nit back, try to provide a good one, and fail again. Interestingly, the models seem to repeat\nthemselves in such a state more often than when thinking (Figure 4). We estimate this\noccurred in 29 R1 responses, 18 Gemini Thinking responses and 3 o1-mini responses.\n7\nOkay, let’s try to figure out this riddle. . . .\nFirst, I need to think of categories that are common knowledge and have exactly 7 members. Some\nexamples that come to mind are days of the week, colors of the rainbow, continents, dwarves in Snow\nWhite, etc. Let’s go through them one by one.\nDays of the week: . . .\nColors of the rainbow: . . .\nContinents: There are 7 continents. Let’s list them: Africa, Antarctica, Asia, Europe, North America,\nSouth America, Australia. . . . Yes, all ending with A except Europe (E). So when sorted by their ending\nletters, Europe would be last since E comes after A. So the last letter alphabetically is E. That fits the\nclue. But I need to check if this is the intended answer. Alternatively, maybe another category. . . .\nFigure 5: R1 finds the answer to Challenge 11 after outputting only 1/4 of its thoughts.\nChallenge Ground Truth Answer\nThe word NONUNION has four N’s and no other\nconsonant. What famous American of the past\n– first and last names, 8 letters in all – has four\ninstances of the same consonant and no other\nconsonant?Eli Lilly.\n(10)\nWe also observe the models may find a good answer early during their thoughts and prefer\nto explore other options before outputting it; we found 10 such R1 responses. This may\nhappen for seemingly understandable reasons: the model may say it’s unsure if a city or\na word is well-known enough. Sometimes the model offers a nonsensical explanation:\n“But does ’Santa’ rhyme with ’Manta’? Not exactly. The first syllables are different: ’San’\nvs. ’Man’. The second syllable is ’ta’ in both, so maybe it’s considered a rhyme? In some\naccents, maybe. But I’m not sure if that’s the intended answer.” And sometimes it finds\nthe correct answer immediately, verifies it, and proceeds to explore other options for no\napparent reason before finally committing to the correct answer (Figure 5).\nChallenge Ground Truth Answer\nThink of a well-known category with exactly 7\nthings in it. Alphabetize the things from their\nending letters, and the last letter alphabetically\nwill be E. In other words, no thing in this category\nends in a letter after E in the alphabet. It’s a\ncategory and set of 7 things that everyone knows.\nWhat is it?Continents.\n(11)\n4.5 How Much Reasoning Is Necessary?\nHow much reasoning is necessary? Since we can access the reasoning output from R1 and\nFlash Thinking, we can empirically determine a “reasoning budget” that is finer-grained\nthan the three “reasoning effort” settings presented offered by the OpenAI API. Figure\n6a depicts the distribution of reasoning-output lengths on the Sunday Puzzle Challenge,\nshowing that most attempts generate fewer than 20,000 tokens. Figure 6b then shows\naccuracy as a function of reasoning length for both models. Gemini Thinking achieves its\naccuracy plateau at roughly 10,000 tokens, whereas R1’s accuracy continues to improve and\nsurpasses Gemini Thinking at around 3,000 tokens.\nThus we conclude that 10,000 tokens of reasoning is unlikely to significantly improve\naccuracy with either model. Consequently, when the model’s reasoning output surpasses\n10,000 tokens, we are likely better off interrupting generation rather than waiting longer\n(and spending more). A similar approach that quantifies how accuracy improves without\nreasoning length is likely helpful for other tasks as well.\n8\n0 20000 40000 60000 80000\nReasoning Length (correct and incorrect)0.00.20.40.60.81.0ProportionModel\nr1\ngemini2ft(a) Reasoning length.\n0 10000 20000 30000\nReasoning Length0.000.050.100.150.200.250.30Accuracy\nModel\nr1\ngemini2ft (b) Accuracy by reasoning length.\nFigure 6: The R1 and Gemini Thinking models allow us to observe their reasoning output,\nand Figure 6a shows the proportion of challenges ( x-axis) with length ( y-axis) indicated.\nThe plot shows that most challenges produce fewer than 20,000 reasoning output tokens.\nFigure 6b shows accuracy by reasoning length. For Gemini Thinking, accuracy reaches a\nplateau at ≈10,000 reasoning output tokens, whereas the plateau appears later for R1. With\na budget of ≈3,000 reasoning output tokens, R1 starts to outperform Gemini Thinking.\n5 Conclusion\nWe present a benchmark for reasoning models with questions that do not require PhD-level\nknowledge, but instead exercise U.S.-centric general knowledge. The benchmark problems\nare challenging for both humans and models, but humans can easily verify the correctness\nof an answer, and models’ mistakes are also obvious.\nWe uncover new failure modes in reasoning models. We observe that models can “give\nup” on a difficult problem and deliberately return an incorrect answer. In rare cases, R1\ncan get stuck “thinking forever”. Finally, by examining the reasoning outputs of R1 and\nGemini Thinking, we quantify the effectiveness of reasoning longer, allowing us to set a\ntoken budget beyond which accuracy reaches a plateau on these tasks. Our work identifies\nnuances of model behavior on accessible yet challenging benchmarks, and also point to\nareas for improvement reasoning models.\nReferences\nFran c ¸ois Chollet. On the Measure of Intelligence, November 2019. URL http://arxiv.org/\nabs/1911.01547 . arXiv:1911.01547 [cs].\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\nJohn Schulman. Training Verifiers to Solve Math Word Problems, November 2021. URL\nhttp://arxiv.org/abs/2110.14168 . arXiv:2110.14168 [cs].\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,\nZ. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan\nWang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong\nDai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu,\nHaocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong\nGuo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.\nCai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin\nHuang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang,\nLei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun\nWang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu\n9\nChen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu\nWang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu,\nShengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao,\nWen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An,\nXiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin,\nXiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou,\nXianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao\nLi, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong,\nYing He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,\nYuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo,\nYuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui\nLi, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren,\nZehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,\nZhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie,\nZiyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\nJanuary 2025. URL http://arxiv.org/abs/2501.12948 . arXiv:2501.12948 [cs].\nGoogle. Gemini 2.0 Flash Thinking Experimental, December 2024. URL https://deepmind.\ngoogle/technologies/gemini/flash-thinking/ .\nAryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno de Moraes\nDumont, and Sanmi Koyejo. Putnam-AXIOM: A Functional and Static Benchmark for\nMeasuring Higher Level Mathematical Reasoning. In The 4th Workshop on Mathematical\nReasoning and AI at NeurIPS’24 , October 2024. URL https://openreview.net/forum?id=\nYXnwlZe0yf&noteId=yrsGpHd0Sf .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric\nTang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem\nSolving With the MATH Dataset. In Joaquin Vanschoren and Sai-Kit Yeung\n(eds.), Proceedings of the Neural Information Processing Systems Track on Datasets\nand Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, Virtual ,\n2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html .\nBowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su,\nCamillo J. Taylor, and Dan Roth. A Peek into Token Bias: Large Language Models Are\nNot Yet Genuine Reasoners, 2024. URL https://doi.org/10.48550/arXiv.2406.11050 .\nYifan Jiang, Filip Ilievski, Kaixin Ma, and Zhivar Sourati. BRAINTEASER: Lateral Thinking\nPuzzles for Large Language Models. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , pp. 14317–14332, Singapore, 2023. Association\nfor Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.885. URL https:\n//aclanthology.org/2023.emnlp-main.885 .\nBin Lei, Yi Zhang, Shan Zuo, Ali Payani, and Caiwen Ding. MACM: Utilizing a Multi-Agent\nSystem for Condition Mining in Solving Complex Mathematical Problems. Technical\nReport arXiv:2404.04735, arXiv, July 2024. URL http://arxiv.org/abs/2404.04735 .\nIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and\nMehrdad Farajtabar. GSM-Symbolic: Understanding the Limitations of Mathematical\nReasoning in Large Language Models, October 2024. URL http://arxiv.org/abs/2410.\n05229 .\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Ha-\njishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand `es, and Tatsunori Hashimoto.\ns1: Simple test-time scaling, January 2025. URL http://arxiv.org/abs/2501.19393 .\narXiv:2501.19393 [cs].\n10\nOpenAI. OpenAI o1 System Card, 2024. URL https://openai.com/index/\nopenai-o1-system-card/ .\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi,\nMichael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren,\nJason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue, Alexandr Wang, and Dan\nHendrycks. Humanity’s Last Exam, 2025.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A Graduate-Level Google-Proof\nQ&A Benchmark. August 2024. URL https://openreview.net/forum?id=Ti67584b98#\ndiscussion .\nJoshua Rozner, Christopher Potts, and Kyle Mahowald. Decrypting Cryptic Crosswords:\nSemantically Complex Wordplay Puzzles as a Target for NLP. November 2021. URL\nhttps://openreview.net/forum?id=Ah5CMODl52 .\nJingmiao Zhao and Carolyn Jane Anderson. Solving and Generating NPR Sunday Puzzles\nwith Large Language Models. In Conference on Computational Creativity (ICCC) , 2023.\nQihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, and Bo Du. Achieving\n>97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for\nMath Word Problems. Technical Report arXiv:2404.14963, arXiv, October 2024. URL\nhttp://arxiv.org/abs/2404.14963 .\n11\n",
  "date_processed": "2025-02-15T21:05:09.887584",
  "source_file": "preprocess\\LLM Reasoning NO PhD.pdf",
  "provider": "openai",
  "model": null
}